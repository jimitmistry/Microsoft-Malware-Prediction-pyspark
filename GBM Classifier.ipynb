{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for running spark locally\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.ml import feature, evaluation, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete or change this cell\n",
    "\n",
    "# grading import statements\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "import os\n",
    "\n",
    "# Define a function to determine if we are running on data bricks\n",
    "# Return true if running in the data bricks environment, false otherwise\n",
    "def is_databricks():\n",
    "    # get the databricks runtime version\n",
    "    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "    \n",
    "    # if running on data bricks\n",
    "    if db_env != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to read the data file.  The full path data file name is constructed\n",
    "# by checking runtime environment variables to determine if the runtime environment is \n",
    "# databricks, or a student's personal computer.  The full path file name is then\n",
    "# constructed based on the runtime env.\n",
    "# \n",
    "# Params\n",
    "#   data_file_name: The base name of the data file to load\n",
    "# \n",
    "# Returns the full path file name based on the runtime env\n",
    "#\n",
    "# Correct Usage Example (pass ONLY the full file name):\n",
    "#   file_name_to_load = get_training_filename(\"sms_spam.csv\") # correct - pass ONLY the full file name  \n",
    "#   \n",
    "# Incorrect Usage Example\n",
    "#   file_name_to_load = get_training_filename(\"/sms_spam.csv\") # incorrect - pass ONLY the full file name\n",
    "#   file_name_to_load = get_training_filename(\"sms_spam.csv/\") # incorrect - pass ONLY the full file name\n",
    "#   file_name_to_load = get_training_filename(\"c:/users/will/data/sms_spam.csv\") incorrect -pass ONLY the full file name\n",
    "def get_training_filename(data_file_name):    \n",
    "    # if running on data bricks\n",
    "    if is_databricks():\n",
    "        # build the full path file name assuming data brick env\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "    # else the data is assumed to be in the same dir as this notebook\n",
    "    else:\n",
    "        # Assume the student is running on their own computer and load the data\n",
    "        # file from the same dir as this notebook\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "    # return the full path file name to the caller\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_1 = sc.textFile(get_training_filename('train_lbl.csv')).map(lambda line: line.split(\",\")).map(lambda x: x[:])\n",
    "\n",
    "header = temp_1.first()\n",
    "\n",
    "micro_df = temp_1.filter(lambda x: x!=header).toDF(header)\n",
    "\n",
    "print((micro_df.count(), len(micro_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in micro_df.columns:\n",
    "    micro_df = micro_df.withColumn(i,micro_df[i].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [i for i in micro_df.columns if i not in ['label']]\n",
    "va = VectorAssembler(inputCols = input_cols, outputCol = 'features')\n",
    "\n",
    "\n",
    "gbt = GBTClassifier()\n",
    "gbt_pipeline = Pipeline(stages=[va, gbt])\n",
    "gbt_fitted = gbt_pipeline.fit(training_df)\n",
    "\n",
    "bce = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "\n",
    "#bce.evaluate(gbt_fitted.transform(validation_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(gbt.maxDepth, [5,10,20]).\\\n",
    "    addGrid(gbt.stepSize, [0.1, 0.2]).\\\n",
    "    addGrid(gbt.maxIter, [20,30]).\\\n",
    "    addGrid(gbt.seed, [5]).\\\n",
    "    build()\n",
    "#addGrid(gbt.maxBins, [32, 48]).\\\n",
    "    #addGrid(gbt.minInstancesPerNode, [1,2,3]).\\\n",
    "    #addGrid(gbt.minInfoGain, [0, 0.1]).\\\n",
    "\n",
    "cv_gbt = CrossValidator(estimator = gbt_pipeline,\n",
    "                          estimatorParamMaps = param_grid,\n",
    "                          evaluator = BinaryClassificationEvaluator(labelCol = 'label',rawPredictionCol = 'rawPrediction', metricName = 'areaUnderROC'),\n",
    "                          numFolds = 3)\n",
    "cvModel_gbt = cv_gbt.fit(micro_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Area Under ROC: {}\".format(np.max(cvModel_gbt.avgMetrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_gbt.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = [ cvModel_gbt.bestModel.stages[-1].featureImportances[i] for i in range(0,len(cvModel_gbt.bestModel.stages[-1].featureImportances))]\n",
    "\n",
    "df_feature = pd.DataFrame(zip(micro_df.columns[:-1], feature_imp), columns = ['feature', 'importance'])\n",
    "df_feature.sort_values('importance', ascending = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
